# -*- coding: utf-8 -*-
"""Deepfake_detector_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_bnI2Q1gp5Rc5g9ugYZ-s4nBgyG7Ukas
"""

from google.colab import files
uploaded = files.upload()

import os

print("Files in /content:")
print(os.listdir("/content"))

import torch

state = torch.load("/content/efficientnet_model.pt", map_location="cpu")
print(state.keys())

import torch
import torch.nn as nn
from torchvision.models import efficientnet_b0

# -------------------------------
# 1. Define model architecture to match checkpoint keys
# -------------------------------
class DeepfakeDetector(nn.Module):
    def __init__(self):
        super().__init__()
        base = efficientnet_b0(weights=None)

        # Match checkpoint keys exactly
        self.features = base.features
        self.avgpool = nn.AdaptiveAvgPool2d(1)

        # Their checkpoint classifier has: classifier.1.weight (Linear)
        self.classifier = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(1280, 2)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x


# -------------------------------
# 2. Load checkpoint
# -------------------------------
model = DeepfakeDetector()

state = torch.load("best_model-v3.pt", map_location="cpu")

# Load EXACT keys (no renaming, no prefix problems)
model.load_state_dict(state, strict=True)

model.eval()

print(" Model loaded successfully!")

!pip install opencv-python

from google.colab import files
uploaded = files.upload()

!pip install ultralytics

!wget https://github.com/akanametov/yolov8-face/releases/download/v0.0.0/yolov8n-face.pt -O yolov8n-face.pt

from ultralytics import YOLO
face_detector = YOLO("/content/yolov8n-face.pt")
print("Face detector loaded ")

import cv2
import torch
import numpy as np
from ultralytics import YOLO
from torchvision import transforms
from PIL import Image
import math

# ----------------------
# LOAD MODELS
# ----------------------
face_detector = YOLO("/content/yolov8n-face.pt")

model = DeepfakeDetector()
model.load_state_dict(torch.load("/content/efficientnet_model.pt", map_location="cpu"))
model.eval()

# ----------------------
# PREPROCESSOR
# ----------------------
preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

def classify_face_crop(face_img):
    img = Image.fromarray(face_img).convert("RGB")
    x = preprocess(img).unsqueeze(0)

    with torch.no_grad():
        logits = model(x)
        probs = torch.softmax(logits, dim=1)[0]

    return probs[1].item()  # fake prob


# ----------------------
# PARAMETERS FOR IMPROVEMENT
# ----------------------
CONF_THRESH = 0.5
MIN_FACE = 80
alpha = 0.30
DIST_THRESH = 80

tracks = {}
next_id = 0


# ----------------------
# SIMPLE TRACKER
# ----------------------
def assign_track_id(center):
    global next_id, tracks

    if len(tracks) == 0:
        tracks[next_id] = {"center": center, "smooth": 0.0}
        next_id += 1
        return next_id - 1

    best_id = None
    best_dist = 9999

    for tid, data in tracks.items():
        cx, cy = data["center"]
        dist = math.dist(center, (cx, cy))

        if dist < best_dist and dist < DIST_THRESH:
            best_dist = dist
            best_id = tid

    if best_id is not None:
        return best_id

    tracks[next_id] = {"center": center, "smooth": 0.0}
    next_id += 1
    return next_id - 1


# ----------------------
# MAIN VIDEO ANALYSIS
# ----------------------
def predict_video(video_path, frame_step=10, max_frames=60):
    global tracks
    tracks = {}

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("Could not open video")
        return

    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frames_processed = 0

    print("Processing video...\n")

    for i in range(0, frame_count, frame_step):

        cap.set(cv2.CAP_PROP_POS_FRAMES, i)
        ret, frame = cap.read()
        if not ret:
            continue

        detections = face_detector(frame)[0]

        for det in detections.boxes:

            if det.conf < CONF_THRESH:
                continue

            x1, y1, x2, y2 = det.xyxy[0].cpu().numpy().astype(int)
            w, h = (x2 - x1), (y2 - y1)

            if w < MIN_FACE or h < MIN_FACE:
                continue

            face = frame[y1:y2, x1:x2]
            if face.size == 0:
                continue

            fake_prob = classify_face_crop(face)

            cx, cy = x1 + w//2, y1 + h//2
            tid = assign_track_id((cx, cy))

            old = tracks[tid]["smooth"]
            smooth = old*(1-alpha) + fake_prob*(alpha)

            tracks[tid]["center"] = (cx, cy)
            tracks[tid]["smooth"] = smooth

        frames_processed += 1
        if frames_processed >= max_frames:
            break

    cap.release()


    # ----------------------
    # FINAL DECISION
    # ----------------------
    if len(tracks) == 0:
        print(" No valid faces found")
        return

    main_face_id = max(tracks.keys())
    final_fake_prob = tracks[main_face_id]["smooth"]

    print("\n===== FINAL RESULT =====")
    print(f"Smoothed Fake Probability: {final_fake_prob:.4f}")

    if final_fake_prob > 0.5:
        print("DEEPFAKE DETECTED")
    else:
        print("REAL VIDEO")

def classify_face_crop(face_img):
    img = Image.fromarray(face_img).convert("RGB")
    x = preprocess(img).unsqueeze(0)

    with torch.no_grad():
        logits = model(x)
        probs = torch.softmax(logits, dim=1)[0]

    return probs[1].item()  # fake probability

face_detector = YOLO("yolov8n-face.pt")
face_detector.overrides['verbose'] = False

def predict_video(video_path, frame_step=15, max_frames=30):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("Could not open video")
        return

    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    results = []
    frames_processed = 0

    # -------------------------------------------------
    # TEMPORAL SMOOTHING + FPS INITIALIZATION
    # -------------------------------------------------
    smoothed_prob = 0.0
    alpha = 0.2
    prob_history = []

    import time
    prev_time = time.time()

    print("Processing video...")

    # -------------------------------------------------
    # MAIN VIDEO LOOP
    # -------------------------------------------------
    for i in range(0, frame_count, frame_step):

        cap.set(cv2.CAP_PROP_POS_FRAMES, i)
        ret, frame = cap.read()
        if not ret:
            continue

        # Face detection
        detections = face_detector(frame)[0]

        for det in detections.boxes:
            x1, y1, x2, y2 = det.xyxy[0].cpu().numpy().astype(int)

            face = frame[y1:y2, x1:x2]
            if face.size == 0:
                continue

            # Classify
            fake_prob = classify_face_crop(face)

            # -------------------------------
            # TEMPORAL SMOOTHING UPDATE
            # -------------------------------
            smoothed_prob = (alpha * fake_prob) + ((1 - alpha) * smoothed_prob)
            prob_history.append(smoothed_prob)

        # -------------------------------
        # FPS UPDATE
        curr_time = time.time()
        fps = 1 / (curr_time - prev_time)
        prev_time = curr_time
        # -------------------------------

        frames_processed += 1

        # -------------------------------
        # CLEAN ONE-LINE LIVE TERMINAL PRINT
        print(f"Frame {frames_processed} | Smoothed Fake: {smoothed_prob:.3f} | FPS: {fps:.1f}", end="\r")
        # -------------------------------

        if frames_processed >= max_frames:
            break

    cap.release()

    # -------------------------------------------------
    # FINAL RESULT
    # -------------------------------------------------
    if not prob_history:
        print("\n No faces detected in video")
        return

    avg_fake = np.mean(prob_history)

    print("\n\n===== FINAL RESULT =====")
    print(f"Average Fake Probability: {avg_fake:.4f}")

    if avg_fake > 0.5:
        print(" DEEPFAKE DETECTED")
    else:
        print(" REAL VIDEO")

predict_video("/content/000_003.mp4")

!pip install onnx onnxruntime onnxscript

import torch
import torch.onnx as onnx

dummy = torch.randn(1, 3, 224, 224)

torch.onnx.export(
    model,                      # your EfficientNet model
    dummy,
    "efficientnet_b0_deepfake.onnx",
    input_names=['input'],
    output_names=['output'],
    opset_version=12,
    dynamic_axes={'input': {0: 'batch'}, 'output': {0: 'batch'}}
)

print("EfficientNet exported to ONNX")

model.training

from ultralytics import YOLO

model_yolo = YOLO("yolov8n-face.pt")
model_yolo.export(format="onnx")

import os

print(os.listdir("/content"))